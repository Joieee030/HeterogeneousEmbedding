import torch
import torch.nn as nn


class SkipGramModel(nn.Module):

    def init__(self, vocab_size, embedding_dim):
        super(SkipGramModel, self).__init__()
        self.u_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)
        self.v_embeddings = nn.Embedding(vocab_size, embedding_dim, sparse=True)
        self.embedding_dim = embedding_dim
        self.init_embeddings()

    def init_embeddings(self):
        # 初始化嵌入矩阵，范围在-0.5 / embedding_dim到0.5 / embedding_dim之间
        init_range = 0.5 / self.embedding_dim
        # 将u_embeddings的权重矩阵数据在-0.5 / embedding_dim到0.5 / embedding_dim之间均匀分布
        self.u_embeddings.weight.data.uniform_(-init_range, init_range)
        # 将v_embeddings的权重矩阵数据在-0.1和0.1之间均匀分布
        self.v_embeddings.weight.data.uniform_(-0.1, 0.1)

    def forward(self, u_pos, v_pos, v_neg):
        u_emb = self.u_embeddings(u_pos)
        v_emb = self.v_embeddings(v_pos)
        # 点乘
        score = torch.sum(torch.mul(u_emb, v_emb), dim=1)
        # 对数损失，避免梯度消失
        log_target = nn.LogSigmoid(score).squeeze()

        # 负样本得分
        if v_neg is not None:
            v_neg_emb = self.v_embeddings(v_neg)
            neg_score = torch.bmm(v_neg_emb, u_emb.unsqueeze(2)).squeeze()
            neg_score = torch.sum(neg_score, dim=1)
            neg_score = nn.LogSigmoid()(-1 * neg_score).squeeze()
            loss = -torch.mean(log_target + neg_score)
        else:
            loss = -torch.mean(log_target)

        return loss

    def input_embedding(self):
        """
        获取输入的嵌入表示。

        Returns:
            numpy.ndarray: 输入的嵌入表示。
        """
        return self.u_embeddings.weight.data.cpu().numpy()

    def save_embedding(self, file_path):
        pass